# LLM_Ethics_Benchmark: Moral Reasoning Assessment of Large Language Models

An open-source framework for systematically evaluating moral reasoning capabilities in large language models (LLMs).
Overview

## Overview

LLM_Ethics_Benchmark (A Three-Dimensional Assessment System for Evaluating Moral Reasoning in Large Language Models) provides a comprehensive framework for assessing how well large language models understand and apply ethical reasoning across diverse scenarios. As LLMs increasingly influence critical decision-making across various sectors, evaluating their moral reasoning capabilities becomes essential. Our benchmark employs a three-dimensional approach to provide nuanced insights into LLM ethical capabilities.

## ğŸš€ Features

- **Standardized Assessment**: Implements Moral Foundations Questionnaire (MFQ), World Values Survey (WVS), and Moral Dilemmas  
- **Multiple LLM Support**: Evaluate Claude, GPT-4, and other models with a consistent methodology  
- **Quantitative Metrics**: Calculate alignment scores based on validated ground truth data  
- **Reasoning Analysis**: Assess the quality and consistency of moral reasoning, not just answers


## Installation

```bash
# Set your API keys as environment variables
export ANTHROPIC_API_KEY="your_api_key_here"
export OPENAI_API_KEY="your_api_key_here"


## ğŸ“Š Key Components

### 1. Moral Foundations Assessment
Evaluates LLM alignment with five fundamental moral dimensions:
* Care/Harm
* Fairness/Cheating
* Loyalty/Betrayal
* Authority/Subversion
* Sanctity/Degradation

### 2. Reasoning Quality Evaluation
Examines the sophistication of moral reasoning through:
* Principle identification
* Perspective-taking abilities
* Consequence analysis
* Consistent principle application

### 3. Cross-cultural Value Consistency
Assesses how LLMs maintain ethical consistency across diverse cultural contexts and scenarios.

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/morals.git
cd morals

# Create and activate a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy the example config and edit it
cp config.yaml.example config.yaml
# Edit config.yaml with your preferred text editor

export ANTHROPIC_API_KEY="your_api_key_here"
export OPENAI_API_KEY="your_api_key_here"

# Run evaluation with Claude on 5 MFQ questions
python -m morals.cli.main --provider anthropic --limit 5

# Evaluate a specific moral foundation
python -m morals.cli.main --foundation care

# Use a different model (e.g., GPT-4)
python -m morals.cli.main --provider openai --model gpt-4

morals/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ instruments/
â”‚       â”œâ”€â”€ mfq.json
â”‚       â”œâ”€â”€ wvs.json
â”‚       â””â”€â”€ dilemmas.json
â”‚
â”œâ”€â”€ morals/
â”‚   â”œâ”€â”€ instruments/
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ mfq.py
â”‚   â”‚   â”œâ”€â”€ wvs.py
â”‚   â”‚   â””â”€â”€ dilemmas.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ anthropic.py
â”‚   â”‚   â”œâ”€â”€ openai.py
â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ processor.py
â”‚   â”‚   â”œâ”€â”€ mfq_evaluator.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ cli/
â”‚       â””â”€â”€ main.py
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ config.yaml.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

import asyncio
from morals.instruments.mfq import MoralFoundationsQuestionnaire
from morals.llm.factory import LLMFactory
from morals.evaluation.mfq_evaluator import MFQEvaluator

async def evaluate_sample():
    mfq = MoralFoundationsQuestionnaire(data_path="data/instruments/mfq.json")
    llm = LLMFactory.create(provider="anthropic")
    evaluator = MFQEvaluator(mfq)

    question_id = "care_r1"
    prompt = mfq.get_prompt_for_question(question_id)
    response = await llm.generate_response(prompt)
    result = evaluator.evaluate_response(question_id, response)
    print(f"Alignment score: {result['alignment_score']}")

if __name__ == "__main__":
    asyncio.run(evaluate_sample())

@misc{morals2025,
  author = {Your Name},
  title = {MORALS: Moral Reasoning Assessment of Large Language Models},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/yourusername/morals}
}


## ğŸ“ Citation

```bibtex
@misc{llm_ethics_benchmark2025,
 author = {Jiao, Junfeng and Murali, Abhejay and Atkinson, David and Afroogh, Saleh and Chen, Kevin and Dhurandhar, Amit},
 title = {LLM\_Ethics\_Benchmark: A Three-Dimensional Assessment System for Evaluating Moral Reasoning in Large Language Models},
 year = {2025},
 publisher = {GitHub},
 url = {https://github.com/yourusername/LLM_Ethics_Benchmark}
}




